apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    enable-auth: "false"
    enable-route: "false"
    maxLoadingConcurrency: "2"
    opendatahub.io/template-display-name: Triton runtime 23.05 - added on 20230630
    opendatahub.io/template-name: triton-23.05-20230630
    openshift.io/display-name: fedora-detection-gpu
  labels:
    name: fedora-detection-gpu
    opendatahub.io/dashboard: "true"
  name: fedora-detection-gpu
  namespace: fedora-detection-prod
spec:
  tolerations:
    - effect: NoSchedule
      key: ssa-team-france.redhat.com/gpu
      operator: Exists
  nodeSelector:
    gpu: nvidia-t4
  builtInAdapter:
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 90000
    runtimeManagementPort: 8001
    serverType: triton
  containers:
  - args:
    - -c
    - 'mkdir -p /models/_triton_models; chmod 777 /models/_triton_models; exec tritonserver
      "--model-repository=/models/_triton_models" "--model-control-mode=explicit"
      "--strict-model-config=false" "--strict-readiness=false" "--allow-http=true"
      "--allow-sagemaker=false" '
    command:
    - /bin/sh
    image: nvcr.io/nvidia/tritonserver:23.05-py3
    livenessProbe:
      exec:
        command:
        - curl
        - --fail
        - --silent
        - --show-error
        - --max-time
        - "9"
        - http://localhost:8000/v2/health/live
      initialDelaySeconds: 5
      periodSeconds: 30
      timeoutSeconds: 10
    name: triton
    resources:
      limits:
        cpu: "2"
        memory: 8Gi
        nvidia.com/gpu: 1
      requests:
        cpu: "1"
        memory: 4Gi
  grpcDataEndpoint: port:8001
  grpcEndpoint: port:8085
  multiModel: true
  protocolVersions:
  - grpc-v2
  replicas: 1
  supportedModelFormats:
  - autoSelect: true
    name: keras
    version: "2"
  - autoSelect: true
    name: onnx
    version: "1"
  - autoSelect: true
    name: pytorch
    version: "1"
  - autoSelect: true
    name: tensorflow
    version: "1"
  - autoSelect: true
    name: tensorflow
    version: "2"
  - autoSelect: true
    name: tensorrt
    version: "7"
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: fedora-detection-gpu
    serving.kserve.io/deploymentMode: ModelMesh
  labels:
    name: fedora-detection-gpu
    opendatahub.io/dashboard: "true"
  name: fedora-detection-gpu
  namespace: fedora-detection-prod
spec:
  predictor:
    model:
      modelFormat:
        name: onnx
        version: "1"
      runtime: fedora-detection-gpu
      storage:
        key: aws-connection-s3-creds
        path: models/registry/default/model.onnx