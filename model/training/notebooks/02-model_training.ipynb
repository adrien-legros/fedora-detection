{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae4ec38-5a70-4288-941a-7f7b7f712659",
   "metadata": {},
   "source": [
    "# Model training with YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e58114-57d5-4731-aa88-7e04ecd7f505",
   "metadata": {},
   "source": [
    "## Prepare YOLO\n",
    "\n",
    "This step is pretty easy, we only have to download YOLOv5 from Ultralytics and install the necessary dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09241b6-8f13-4e20-bfd2-74afee1170b4",
   "metadata": {},
   "source": [
    "### Download YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68197c4e-f6c3-48d0-b066-5726e6a055e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 16074, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 16074 (delta 5), reused 9 (delta 1), pack-reused 16056\u001b[K\n",
      "Receiving objects: 100% (16074/16074), 14.69 MiB | 21.31 MiB/s, done.\n",
      "Resolving deltas: 100% (11033/11033), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89c6eb-98c7-4e9d-8756-e85a18af21c0",
   "metadata": {},
   "source": [
    "### We must slightly edit the requirements to have a container-compatible version of openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07282081-61c0-499a-959d-710b477e9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/opencv-python/opencv-python-headless/' yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e215b-7473-45a4-a91a-97b335546320",
   "metadata": {},
   "source": [
    "### Then install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8123c9c-80a7-4831-9bc7-b67a309fcd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qr yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d72a7-04eb-4f22-b618-84d18f3e05d5",
   "metadata": {},
   "source": [
    "### Unfortunately, some subdependencies may have messed up with OpenCV. So let's make sure again we have the right version again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b358c19d-c4d2-4562-bfdc-5884d437d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qy opencv-python\n",
    "!pip uninstall -qy opencv-python-headless\n",
    "!pip install -q opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a490a-ad68-4bc5-afc5-cfdd048a93e9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae012df-92f7-4fc0-b012-2159ccdd2139",
   "metadata": {},
   "source": [
    "### Create the configuration file\n",
    "\n",
    "There is a `configuration.yaml` file already present in the folder. Verify that it has the right number and names of the classes you want to use and that you downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ddbf6-0e6e-49c0-ba40-eb9108beabd3",
   "metadata": {},
   "source": [
    "### Freeze the YOLOv5 Backbone\n",
    "The backbone means the layers that extract input image features. We will freeze the backbone so the weights in the backbone layers will not change during YOLOv5 transfer learning. We will then only train the last layers (the head layers).\n",
    "\n",
    "In this example, we will use the smallest available base model, yolov5n. If we open the file `yolov5/models/yolov5n.yaml` we can see the following structure:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9865eca3-13ab-4c93-bf49-d08442a58925",
   "metadata": {},
   "source": [
    "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
    "\n",
    "# Parameters\n",
    "nc: 80  # number of classes\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.25  # layer channel multiple\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv5 v6.0 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
    "   [-1, 3, C3, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
    "   [-1, 6, C3, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
    "   [-1, 9, C3, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
    "   [-1, 3, C3, [1024]],\n",
    "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
    "  ]\n",
    "\n",
    "# YOLOv5 v6.0 head\n",
    "head:\n",
    "  [[-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 3, C3, [512, False]],  # 13\n",
    "\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
    "\n",
    "   [-1, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
    "\n",
    "   [-1, 1, Conv, [512, 3, 2]],\n",
    "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
    "\n",
    "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8e6fd-ac8a-4021-9827-b189837b9e89",
   "metadata": {},
   "source": [
    "In the **backbone** section we can see that there are 10 layers. So that's the number of layers we want to freeze in our training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcfae3-e88f-42e9-9c69-7502f5f97acf",
   "metadata": {},
   "source": [
    "### Some training parameters (hyperparameters) we are going to use\n",
    "\n",
    "#### The base model\n",
    "weights = yolov5n.pt\n",
    "\n",
    "This is the smallest available model, to save processing time in this example. Adjust for better results.\n",
    "Other models are available here: https://github.com/ultralytics/yolov5#pretrained-checkpoints\n",
    "\n",
    "####  The number of training iterations\n",
    "epochs = 50\n",
    "\n",
    "This is voluntarily low to save processing time for this example. Adjust for better results (>150).\n",
    "\n",
    "#### The number of images analyzed in a single pass\n",
    "batch = 256\n",
    "\n",
    "You may have to adjust this depending on the memory available on your GPU. The higher (in a power of 2) the better, until you run out of memory...\n",
    "\n",
    "#### Number of layers to freeze\n",
    "freeze = 10\n",
    "\n",
    "As per above, we want to freeze 10 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2231bb5-081a-4e98-872b-89073a80ef18",
   "metadata": {},
   "source": [
    "### We can now launch the training!\n",
    "\n",
    "NOTE: PyTorch first caches images to speed up the process. If you have enough memory and shared memory that is not an issue. However, this may not be always the case, especially with large datasets. Therefore the cache is forced to disk here. But of course this is a parameter you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7463168-e2e1-429d-92e4-86486059afa8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=configuration.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=disk, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[10], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
      "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.9.16 torch-1.13.1+cu117 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "freezing model.0.conv.weight\n",
      "freezing model.0.bn.weight\n",
      "freezing model.0.bn.bias\n",
      "freezing model.1.conv.weight\n",
      "freezing model.1.bn.weight\n",
      "freezing model.1.bn.bias\n",
      "freezing model.2.cv1.conv.weight\n",
      "freezing model.2.cv1.bn.weight\n",
      "freezing model.2.cv1.bn.bias\n",
      "freezing model.2.cv2.conv.weight\n",
      "freezing model.2.cv2.bn.weight\n",
      "freezing model.2.cv2.bn.bias\n",
      "freezing model.2.cv3.conv.weight\n",
      "freezing model.2.cv3.bn.weight\n",
      "freezing model.2.cv3.bn.bias\n",
      "freezing model.2.m.0.cv1.conv.weight\n",
      "freezing model.2.m.0.cv1.bn.weight\n",
      "freezing model.2.m.0.cv1.bn.bias\n",
      "freezing model.2.m.0.cv2.conv.weight\n",
      "freezing model.2.m.0.cv2.bn.weight\n",
      "freezing model.2.m.0.cv2.bn.bias\n",
      "freezing model.3.conv.weight\n",
      "freezing model.3.bn.weight\n",
      "freezing model.3.bn.bias\n",
      "freezing model.4.cv1.conv.weight\n",
      "freezing model.4.cv1.bn.weight\n",
      "freezing model.4.cv1.bn.bias\n",
      "freezing model.4.cv2.conv.weight\n",
      "freezing model.4.cv2.bn.weight\n",
      "freezing model.4.cv2.bn.bias\n",
      "freezing model.4.cv3.conv.weight\n",
      "freezing model.4.cv3.bn.weight\n",
      "freezing model.4.cv3.bn.bias\n",
      "freezing model.4.m.0.cv1.conv.weight\n",
      "freezing model.4.m.0.cv1.bn.weight\n",
      "freezing model.4.m.0.cv1.bn.bias\n",
      "freezing model.4.m.0.cv2.conv.weight\n",
      "freezing model.4.m.0.cv2.bn.weight\n",
      "freezing model.4.m.0.cv2.bn.bias\n",
      "freezing model.4.m.1.cv1.conv.weight\n",
      "freezing model.4.m.1.cv1.bn.weight\n",
      "freezing model.4.m.1.cv1.bn.bias\n",
      "freezing model.4.m.1.cv2.conv.weight\n",
      "freezing model.4.m.1.cv2.bn.weight\n",
      "freezing model.4.m.1.cv2.bn.bias\n",
      "freezing model.5.conv.weight\n",
      "freezing model.5.bn.weight\n",
      "freezing model.5.bn.bias\n",
      "freezing model.6.cv1.conv.weight\n",
      "freezing model.6.cv1.bn.weight\n",
      "freezing model.6.cv1.bn.bias\n",
      "freezing model.6.cv2.conv.weight\n",
      "freezing model.6.cv2.bn.weight\n",
      "freezing model.6.cv2.bn.bias\n",
      "freezing model.6.cv3.conv.weight\n",
      "freezing model.6.cv3.bn.weight\n",
      "freezing model.6.cv3.bn.bias\n",
      "freezing model.6.m.0.cv1.conv.weight\n",
      "freezing model.6.m.0.cv1.bn.weight\n",
      "freezing model.6.m.0.cv1.bn.bias\n",
      "freezing model.6.m.0.cv2.conv.weight\n",
      "freezing model.6.m.0.cv2.bn.weight\n",
      "freezing model.6.m.0.cv2.bn.bias\n",
      "freezing model.6.m.1.cv1.conv.weight\n",
      "freezing model.6.m.1.cv1.bn.weight\n",
      "freezing model.6.m.1.cv1.bn.bias\n",
      "freezing model.6.m.1.cv2.conv.weight\n",
      "freezing model.6.m.1.cv2.bn.weight\n",
      "freezing model.6.m.1.cv2.bn.bias\n",
      "freezing model.6.m.2.cv1.conv.weight\n",
      "freezing model.6.m.2.cv1.bn.weight\n",
      "freezing model.6.m.2.cv1.bn.bias\n",
      "freezing model.6.m.2.cv2.conv.weight\n",
      "freezing model.6.m.2.cv2.bn.weight\n",
      "freezing model.6.m.2.cv2.bn.bias\n",
      "freezing model.7.conv.weight\n",
      "freezing model.7.bn.weight\n",
      "freezing model.7.bn.bias\n",
      "freezing model.8.cv1.conv.weight\n",
      "freezing model.8.cv1.bn.weight\n",
      "freezing model.8.cv1.bn.bias\n",
      "freezing model.8.cv2.conv.weight\n",
      "freezing model.8.cv2.bn.weight\n",
      "freezing model.8.cv2.bn.bias\n",
      "freezing model.8.cv3.conv.weight\n",
      "freezing model.8.cv3.bn.weight\n",
      "freezing model.8.cv3.bn.bias\n",
      "freezing model.8.m.0.cv1.conv.weight\n",
      "freezing model.8.m.0.cv1.bn.weight\n",
      "freezing model.8.m.0.cv1.bn.bias\n",
      "freezing model.8.m.0.cv2.conv.weight\n",
      "freezing model.8.m.0.cv2.bn.weight\n",
      "freezing model.8.m.0.cv2.bn.bias\n",
      "freezing model.9.cv1.conv.weight\n",
      "freezing model.9.cv1.bn.weight\n",
      "freezing model.9.cv1.bn.bias\n",
      "freezing model.9.cv2.conv.weight\n",
      "freezing model.9.cv2.bn.weight\n",
      "freezing model.9.cv2.bn.bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /opt/app-root/src/yolov5-transfer-learning/data/labels/train... \u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /opt/app-root/src/yolov5-transfer-learning/data/labels/train.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.4GB disk): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1125/1125 [00:03<00:00, 331\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app-root/src/yolov5-transfer-learning/data/labels/val... 187 \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /opt/app-root/src/yolov5-transfer-learning/data/labels/val.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.4GB disk): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:01<00:00, 100.02i\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.96 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to yolov5/runs/train/exp13/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolov5/runs/train/exp13\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G    0.08634    0.02892          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.299       0.43      0.307     0.0785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G    0.05978    0.02301          0         10        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.527      0.603      0.572      0.231\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G    0.05316    0.01946          0          8        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.495      0.793       0.51      0.218\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G    0.04618    0.01632          0         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.869      0.874      0.885      0.452\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G    0.04096    0.01475          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.883      0.911      0.934      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G    0.03745    0.01372          0          7        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.895       0.92       0.94       0.55\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G    0.03513     0.0131          0         10        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.877      0.902       0.93      0.595\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G    0.03278    0.01289          0         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.881       0.93      0.931      0.562\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G    0.03213    0.01288          0         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.885      0.911      0.944      0.649\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49         0G    0.03114    0.01209          0         10        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.896      0.926      0.949      0.665\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G    0.02836     0.0117          0          9        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214        0.9      0.939      0.943      0.655\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G    0.02774    0.01212          0         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.914      0.911      0.944      0.662\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G    0.02788    0.01203          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.897      0.937      0.951      0.673\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G    0.02712    0.01116          0          9        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.903       0.93      0.932      0.661\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G    0.02754    0.01102          0          6        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.878      0.944       0.94      0.663\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G    0.02593    0.01116          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.913      0.949      0.941      0.688\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G    0.02493    0.01121          0         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.923      0.916      0.949      0.699\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49         0G    0.02443    0.01102          0          8        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.906      0.935      0.945      0.693\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49         0G    0.02422     0.0105          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.879      0.946      0.942      0.693\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G    0.02173     0.0114          0         36        640:  ^C\n",
      "      19/49         0G    0.02173     0.0114          0         36        640:  \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/app-root/src/yolov5-transfer-learning/yolov5/train.py\", line 647, in <module>\n",
      "    main(opt)\n",
      "  File \"/opt/app-root/src/yolov5-transfer-learning/yolov5/train.py\", line 536, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/opt/app-root/src/yolov5-transfer-learning/yolov5/train.py\", line 317, in train\n",
      "    pred = model(imgs)  # forward\n",
      "  File \"/opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/app-root/src/yolov5-transfer-learning/yolov5/models/yolo.py\", line 209, in forward\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
      "  File \"/opt/app-root/src/yolov5-transfer-learning/yolov5/models/yolo.py\", line 121, in _forward_once\n",
      "    x = m(x)  # run\n",
      "  File \"/opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/app-root/src/yolov5-transfer-learning/yolov5/models/common.py\", line 68, in forward\n",
      "    return self.act(self.bn(self.conv(x)))\n",
      "  File \"/opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/opt/app-root/lib64/python3.9/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/train.py --data configuration.yaml --weights yolov5s.pt --epochs 50 --batch 16 --freeze 10 --cache disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacbf16-5971-46f7-b5d6-54f90a7eb88b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5/yolov5s.pt, cfg=, data=yolov5/data/coco128.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=yolov5/runs/train/exp13/weights/last.pt, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 1 commit. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.9.16 torch-1.13.1+cu117 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 349/349 items from yolov5/runs/train/exp13/weights/last.pt\n",
      "freezing model.0.conv.weight\n",
      "freezing model.0.bn.weight\n",
      "freezing model.0.bn.bias\n",
      "freezing model.1.conv.weight\n",
      "freezing model.1.bn.weight\n",
      "freezing model.1.bn.bias\n",
      "freezing model.2.cv1.conv.weight\n",
      "freezing model.2.cv1.bn.weight\n",
      "freezing model.2.cv1.bn.bias\n",
      "freezing model.2.cv2.conv.weight\n",
      "freezing model.2.cv2.bn.weight\n",
      "freezing model.2.cv2.bn.bias\n",
      "freezing model.2.cv3.conv.weight\n",
      "freezing model.2.cv3.bn.weight\n",
      "freezing model.2.cv3.bn.bias\n",
      "freezing model.2.m.0.cv1.conv.weight\n",
      "freezing model.2.m.0.cv1.bn.weight\n",
      "freezing model.2.m.0.cv1.bn.bias\n",
      "freezing model.2.m.0.cv2.conv.weight\n",
      "freezing model.2.m.0.cv2.bn.weight\n",
      "freezing model.2.m.0.cv2.bn.bias\n",
      "freezing model.3.conv.weight\n",
      "freezing model.3.bn.weight\n",
      "freezing model.3.bn.bias\n",
      "freezing model.4.cv1.conv.weight\n",
      "freezing model.4.cv1.bn.weight\n",
      "freezing model.4.cv1.bn.bias\n",
      "freezing model.4.cv2.conv.weight\n",
      "freezing model.4.cv2.bn.weight\n",
      "freezing model.4.cv2.bn.bias\n",
      "freezing model.4.cv3.conv.weight\n",
      "freezing model.4.cv3.bn.weight\n",
      "freezing model.4.cv3.bn.bias\n",
      "freezing model.4.m.0.cv1.conv.weight\n",
      "freezing model.4.m.0.cv1.bn.weight\n",
      "freezing model.4.m.0.cv1.bn.bias\n",
      "freezing model.4.m.0.cv2.conv.weight\n",
      "freezing model.4.m.0.cv2.bn.weight\n",
      "freezing model.4.m.0.cv2.bn.bias\n",
      "freezing model.4.m.1.cv1.conv.weight\n",
      "freezing model.4.m.1.cv1.bn.weight\n",
      "freezing model.4.m.1.cv1.bn.bias\n",
      "freezing model.4.m.1.cv2.conv.weight\n",
      "freezing model.4.m.1.cv2.bn.weight\n",
      "freezing model.4.m.1.cv2.bn.bias\n",
      "freezing model.5.conv.weight\n",
      "freezing model.5.bn.weight\n",
      "freezing model.5.bn.bias\n",
      "freezing model.6.cv1.conv.weight\n",
      "freezing model.6.cv1.bn.weight\n",
      "freezing model.6.cv1.bn.bias\n",
      "freezing model.6.cv2.conv.weight\n",
      "freezing model.6.cv2.bn.weight\n",
      "freezing model.6.cv2.bn.bias\n",
      "freezing model.6.cv3.conv.weight\n",
      "freezing model.6.cv3.bn.weight\n",
      "freezing model.6.cv3.bn.bias\n",
      "freezing model.6.m.0.cv1.conv.weight\n",
      "freezing model.6.m.0.cv1.bn.weight\n",
      "freezing model.6.m.0.cv1.bn.bias\n",
      "freezing model.6.m.0.cv2.conv.weight\n",
      "freezing model.6.m.0.cv2.bn.weight\n",
      "freezing model.6.m.0.cv2.bn.bias\n",
      "freezing model.6.m.1.cv1.conv.weight\n",
      "freezing model.6.m.1.cv1.bn.weight\n",
      "freezing model.6.m.1.cv1.bn.bias\n",
      "freezing model.6.m.1.cv2.conv.weight\n",
      "freezing model.6.m.1.cv2.bn.weight\n",
      "freezing model.6.m.1.cv2.bn.bias\n",
      "freezing model.6.m.2.cv1.conv.weight\n",
      "freezing model.6.m.2.cv1.bn.weight\n",
      "freezing model.6.m.2.cv1.bn.bias\n",
      "freezing model.6.m.2.cv2.conv.weight\n",
      "freezing model.6.m.2.cv2.bn.weight\n",
      "freezing model.6.m.2.cv2.bn.bias\n",
      "freezing model.7.conv.weight\n",
      "freezing model.7.bn.weight\n",
      "freezing model.7.bn.bias\n",
      "freezing model.8.cv1.conv.weight\n",
      "freezing model.8.cv1.bn.weight\n",
      "freezing model.8.cv1.bn.bias\n",
      "freezing model.8.cv2.conv.weight\n",
      "freezing model.8.cv2.bn.weight\n",
      "freezing model.8.cv2.bn.bias\n",
      "freezing model.8.cv3.conv.weight\n",
      "freezing model.8.cv3.bn.weight\n",
      "freezing model.8.cv3.bn.bias\n",
      "freezing model.8.m.0.cv1.conv.weight\n",
      "freezing model.8.m.0.cv1.bn.weight\n",
      "freezing model.8.m.0.cv1.bn.bias\n",
      "freezing model.8.m.0.cv2.conv.weight\n",
      "freezing model.8.m.0.cv2.bn.weight\n",
      "freezing model.8.m.0.cv2.bn.bias\n",
      "freezing model.9.cv1.conv.weight\n",
      "freezing model.9.cv1.bn.weight\n",
      "freezing model.9.cv1.bn.bias\n",
      "freezing model.9.cv2.conv.weight\n",
      "freezing model.9.cv2.bn.weight\n",
      "freezing model.9.cv2.bn.bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "Resuming training from yolov5/runs/train/exp13/weights/last.pt from epoch 19 to 50 total epochs\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /opt/app-root/src/yolov5-transfer-learning/data/labels/train.cac\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.4GB disk): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1125/1125 [00:00<00:00, 205\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app-root/src/yolov5-transfer-learning/data/labels/val.cache..\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.4GB disk): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:00<00:00, 17478.6\u001b[0m\n",
      "Plotting labels to yolov5/runs/train/exp13/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolov5/runs/train/exp13\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G    0.02443    0.01064          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214       0.89      0.947      0.936      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49         0G    0.02413    0.01068          0         10        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214       0.88      0.944      0.937      0.686\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49         0G    0.02384    0.01056          0          8        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.911      0.925      0.946      0.697\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49         0G    0.02291   0.009917          0         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.913       0.93       0.94      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49         0G      0.023   0.009962          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.909      0.935      0.944       0.69\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49         0G    0.02239   0.009736          0          7        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.894      0.958       0.94      0.691\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49         0G    0.02298   0.009801          0         10        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.912      0.944      0.952      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49         0G    0.02186   0.009842          0         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.924      0.916       0.95      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49         0G    0.02194   0.009986          0         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.929      0.914      0.941      0.698\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49         0G    0.02151   0.009452          0         10        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        187        214      0.894      0.958      0.945      0.691\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/49         0G    0.02068   0.009187          0         34        640:  "
     ]
    }
   ],
   "source": [
    "!python yolov5/train.py --resume yolov5/runs/train/exp13/weights/last.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad367ec3-2c18-4532-8ba9-68c2d07b0ba8",
   "metadata": {},
   "source": [
    "The model has been saved under `yolov5/runs/train/exp/weights/best.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245d6cb-a3d4-4322-a781-0c4196ca3762",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "We can now simply test the model against an image: `test/img_test.jpg`\n",
    "The image will be analyzed, enriched with bounding boxes, and the result will be saved under `yolov5/runs/detect/exp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42f9280-ed54-4821-a707-1b78f2eaa75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5/runs/train/exp12/weights/best.pt'], source=test, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.2, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 üöÄ v7.0-240-g84ec8b5 Python-3.9.16 torch-1.13.1+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/4 /opt/app-root/src/yolov5-transfer-learning/test/adrien-redhat.jpg: 640x480 1 redhat, 139.3ms\n",
      "image 2/4 /opt/app-root/src/yolov5-transfer-learning/test/image-1.jpg: 640x640 3 redhats, 127.0ms\n",
      "image 3/4 /opt/app-root/src/yolov5-transfer-learning/test/image-17.jpg: 640x640 2 redhats, 159.3ms\n",
      "image 4/4 /opt/app-root/src/yolov5-transfer-learning/test/image-2.jpg: 640x576 1 redhat, 127.8ms\n",
      "Speed: 0.5ms pre-process, 138.3ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolov5/runs/detect/exp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/detect.py --weights yolov5/runs/train/exp12/weights/best.pt --img 640 --conf-thres 0.2 --source test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
