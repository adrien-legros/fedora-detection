---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: ia-inference
  labels:
    app: ia-inference
    app.kubernetes.io/component: ia-inference
    app.kubernetes.io/instance: ia-inference
    app.kubernetes.io/name: ia-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ia-inference
      app.kubernetes.io/component: ia-inference
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: ia-inference
        app.kubernetes.io/component: ia-inference
    spec:
      containers:
        - name: ia-inference
          image: >-
            quay.io/alegros/browser-fedora-detection:latest
          env:
            - name: GRPC_HOST
              value: 'modelmesh-serving.fedora-detection'
            - name: GRPC_PORT
              value: '8033'
            - name: MODEL_NAME
              value: 'fedora-detection'
            - name: CONF_THRESHOLD
              value: '0.6'
            - name: IOU_THRESHOLD
              value: '0.6'
          envFrom:
          - secretRef:
              name: aws-connection-s3-creds
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: Always
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      securityContext: {}
      schedulerName: default-scheduler
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
---
kind: Service
apiVersion: v1
metadata:
  name: ia-inference
  labels:
    app: ia-inference
    app.kubernetes.io/component: ia-inference
    app.kubernetes.io/instance: ia-inference
    app.kubernetes.io/name: ia-inference
spec:
  ports:
    - name: 8080-tcp
      protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app: ia-inference
    app.kubernetes.io/component: ia-inference
---
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: ia-inference
  labels:
    app: ia-inference
    app.kubernetes.io/component: ia-inference
    app.kubernetes.io/instance: ia-inference
    app.kubernetes.io/name: ia-inference
spec:
  to:
    kind: Service
    name: ia-inference
    weight: 100
  port:
    targetPort: 8080-tcp
  tls:
    termination: edge
  wildcardPolicy: None